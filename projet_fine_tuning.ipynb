{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc6d315-2209-47da-9073-1cad30866a2d",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "Tuto classif avec camembert : https://xiaoouwang.medium.com/classification-de-commentaires-avec-camembert-sans-prise-de-t%C3%AAte-les-fondamentaux-dbf8070f679b  \n",
    "Tuto fine-tuning avec sentence-bert : https://h4c5.hashnode.dev/finetuner-un-modele-camembert-pour-lembedding-de-phrases  \n",
    "Tuto fine-tuning Camembert pour NER : https://www.quantmetry.com/blog/fine-tune-modele-bert-tache-ner/  \n",
    "Article expliquant les Transformers : https://www.hugomichel.io/post/transformer/  \n",
    "Huggingface Camembert : https://huggingface.co/docs/transformers/model_doc/camembert#transformers.CamembertModel  \n",
    "Article sur la fonction de coût pour classification multi-label : https://medium.com/@kitkat73275/multi-label-classification-8d8ae55e8373  \n",
    "Implémentation de la fonction de coût dans un modèle DL : https://machinelearningmastery.com/multi-label-classification-with-deep-learning/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac0b8dd-9b5a-4d1c-b895-76eb24548efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8abc8-29a7-47fa-bd19-de764444f1e9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c96a80-a1d9-4d2a-b262-91920982146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Misc\n",
    "import os\n",
    "import s3fs\n",
    "\n",
    "# pre-trained Model\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d38e-0380-4002-847b-a91a0ff25c73",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1668361-9fee-49ae-ac13-f6350d64ae01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nsimonovici-ensae']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "S3_ENDPOINT_URL  = 'https://'+os.environ['AWS_S3_ENDPOINT']\n",
    "fs = s3fs.S3FileSystem(client_kwargs = {'endpoint_url' : S3_ENDPOINT_URL})\n",
    "fs.ls('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "682a33c4-8af7-41ac-acff-dbce9b0b5134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://nsimonovici-ensae/PROJET/data/project-1-at-2024-12-27-15-41-3fe0002a_prepared.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET = \"nsimonovici-ensae\"\n",
    "DIR_KEY_S3 = \"PROJET/data/\"\n",
    "FILE_KEY_S3 = 'project-1-at-2024-12-27-15-41-3fe0002a_prepared.csv'\n",
    "s3_path = f\"s3a://{BUCKET}/{DIR_KEY_S3}{FILE_KEY_S3}\"\n",
    "s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5424bc06-fe77-41a5-92e7-c5f3d6594e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263 Lignes et 10 Colonnes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inner_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_idx</th>\n",
       "      <th>data.text</th>\n",
       "      <th>data.name</th>\n",
       "      <th>file_upload</th>\n",
       "      <th>id</th>\n",
       "      <th>data.url</th>\n",
       "      <th>data.username</th>\n",
       "      <th>data.date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"social\"]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>Stop #ViolenceAgainstWomen!\\n\\n#OrangeTheWorld...</td>\n",
       "      <td>L’Europe Ensemble</td>\n",
       "      <td>1de050d6-bluesky-comptes-politiques-2_.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>https://bsky.app/profile/fabiennekeller.bsky.s...</td>\n",
       "      <td>ensembleue.bsky.social</td>\n",
       "      <td>2024-11-25 19:25:10.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[\"social\"]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>La violence à l’égard des femmes reste une urg...</td>\n",
       "      <td>L’Europe Ensemble</td>\n",
       "      <td>1de050d6-bluesky-comptes-politiques-2_.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>https://bsky.app/profile/mariepierrev.bsky.soc...</td>\n",
       "      <td>ensembleue.bsky.social</td>\n",
       "      <td>2024-11-25 19:24:48.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"international\",\"économie\"]</td>\n",
       "      <td>[8,3]</td>\n",
       "      <td>« Notre position sur le Mercosur, depuis 2019,...</td>\n",
       "      <td>L’Europe Ensemble</td>\n",
       "      <td>1de050d6-bluesky-comptes-politiques-2_.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>https://bsky.app/profile/ensembleue.bsky.socia...</td>\n",
       "      <td>ensembleue.bsky.social</td>\n",
       "      <td>2024-11-25 13:42:39.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[\"international\",\"économie\",\"social\"]</td>\n",
       "      <td>[8,3,11]</td>\n",
       "      <td>Nouvelle session plénière à Strasbourg ! À l’o...</td>\n",
       "      <td>L’Europe Ensemble</td>\n",
       "      <td>1de050d6-bluesky-comptes-politiques-2_.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>https://bsky.app/profile/ensembleue.bsky.socia...</td>\n",
       "      <td>ensembleue.bsky.social</td>\n",
       "      <td>2024-11-25 12:59:05.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[\"international\",\"immigration\"]</td>\n",
       "      <td>[8,7]</td>\n",
       "      <td>Désaveu cinglant pour Meloni qui se résout à r...</td>\n",
       "      <td>L’Europe Ensemble</td>\n",
       "      <td>1de050d6-bluesky-comptes-politiques-2_.csv</td>\n",
       "      <td>5</td>\n",
       "      <td>https://bsky.app/profile/fabiennekeller.bsky.s...</td>\n",
       "      <td>ensembleue.bsky.social</td>\n",
       "      <td>2024-11-25 12:50:04.455000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inner_id                                  topic topic_idx  \\\n",
       "0         1                             [\"social\"]      [11]   \n",
       "1         2                             [\"social\"]      [11]   \n",
       "2         3           [\"international\",\"économie\"]     [8,3]   \n",
       "3         4  [\"international\",\"économie\",\"social\"]  [8,3,11]   \n",
       "4         5        [\"international\",\"immigration\"]     [8,7]   \n",
       "\n",
       "                                           data.text          data.name  \\\n",
       "0  Stop #ViolenceAgainstWomen!\\n\\n#OrangeTheWorld...  L’Europe Ensemble   \n",
       "1  La violence à l’égard des femmes reste une urg...  L’Europe Ensemble   \n",
       "2  « Notre position sur le Mercosur, depuis 2019,...  L’Europe Ensemble   \n",
       "3  Nouvelle session plénière à Strasbourg ! À l’o...  L’Europe Ensemble   \n",
       "4  Désaveu cinglant pour Meloni qui se résout à r...  L’Europe Ensemble   \n",
       "\n",
       "                                  file_upload  id  \\\n",
       "0  1de050d6-bluesky-comptes-politiques-2_.csv   1   \n",
       "1  1de050d6-bluesky-comptes-politiques-2_.csv   2   \n",
       "2  1de050d6-bluesky-comptes-politiques-2_.csv   3   \n",
       "3  1de050d6-bluesky-comptes-politiques-2_.csv   4   \n",
       "4  1de050d6-bluesky-comptes-politiques-2_.csv   5   \n",
       "\n",
       "                                            data.url           data.username  \\\n",
       "0  https://bsky.app/profile/fabiennekeller.bsky.s...  ensembleue.bsky.social   \n",
       "1  https://bsky.app/profile/mariepierrev.bsky.soc...  ensembleue.bsky.social   \n",
       "2  https://bsky.app/profile/ensembleue.bsky.socia...  ensembleue.bsky.social   \n",
       "3  https://bsky.app/profile/ensembleue.bsky.socia...  ensembleue.bsky.social   \n",
       "4  https://bsky.app/profile/fabiennekeller.bsky.s...  ensembleue.bsky.social   \n",
       "\n",
       "                    data.date  \n",
       "0  2024-11-25 19:25:10.459000  \n",
       "1  2024-11-25 19:24:48.661000  \n",
       "2  2024-11-25 13:42:39.117000  \n",
       "3  2024-11-25 12:59:05.431000  \n",
       "4  2024-11-25 12:50:04.455000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with fs.open(s3_path, mode = 'r') as file_in : \n",
    "    data_raw = pd.read_csv(file_in, sep=\";\")\n",
    "\n",
    "print(f\"{data_raw.shape[0]} Lignes et {data_raw.shape[1]} Colonnes\")\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69c74d-02f4-4267-bfb8-6b5adcbba1f6",
   "metadata": {},
   "source": [
    "# CamemBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139b5084-047a-41f3-95ca-0ed9db14f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-base\")\n",
    "# model = CamembertModel.from_pretrained(\"almanach/camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac12aa3-69d5-48fa-809d-cad14e850bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict().keys() # Affiche les poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7af20-ca09-4fa0-b7e1-5a3050ac477b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for module in model.modules(): # Affiche les 'modules' dont les couches\n",
    "#     print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c9b01-b778-44f4-b2a0-c153b1445c15",
   "metadata": {},
   "source": [
    "## Entraînement ou non des couches cachées\n",
    "Les couches sont toutes trainable de base.  \n",
    "On veut les rendre non-trainable pour le fine-tuning avec un petit jeu de données  \n",
    "Voir cet article qui discute du sujet : https://discuss.huggingface.co/t/the-point-of-using-pretrained-model-if-i-dont-freeze-layers/40675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ddfde1-ffd0-4174-8f15-18d27bdceee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e767e4b-1c57-46a4-b416-dd46a38985c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"num params:\", model.num_parameters())\n",
    "# print(f\"num trainable params:\", model.num_parameters(only_trainable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4813ba9-68ff-45b5-946d-9e3479c92ff9",
   "metadata": {},
   "source": [
    "## Fonction de coût :  \n",
    "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss. Unlike Softmax loss it is independent for each vector component (class), meaning that the loss computed for every CNN output vector component is not affected by other component values. That’s why it is used for multi-label classification, where the insight of an element belonging to a certain class should not influence the decision for another class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953744de-c609-4815-845d-ab3a2792d59c",
   "metadata": {},
   "source": [
    "## Altération du model : couches de fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "100b0b49-b71d-4e5c-87d3-cd4a0d50e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_finetuning_model(n_outputs):\n",
    "# \tmodel = Sequential()\n",
    "# \tmodel.out = Linear(n_outputs, activation='sigmoid')\n",
    "# \tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "# \treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b090f78-10d1-4eef-a24b-1620a224d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fineTuning = get_finetuning_model(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322773c-7657-4d83-a770-0a612512131f",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94feba58-7de6-4153-b494-ae2d259da659",
   "metadata": {},
   "source": [
    "## Chargement du modèle et du tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e810a612-74e3-4457-975b-11e6f0c4e3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=16, problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84abdbc6-fa94-4aeb-ad33-3eea6c587d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params: 110634256\n",
      "num trainable params: 110634256\n"
     ]
    }
   ],
   "source": [
    "print(f\"num params:\", model.num_parameters())\n",
    "print(f\"num trainable params:\", model.num_parameters(only_trainable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f497b-23c3-4c9b-98c7-2019929b10b2",
   "metadata": {},
   "source": [
    "Utilisation des \"named_modules\" pour identifier le classifieur à la tête du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ecccc87-9639-4daf-8e8a-b0e8398bea8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting module ***roberta*** non-trainable\n",
      "Setting module ***roberta.embeddings*** non-trainable\n",
      "Setting module ***roberta.embeddings.word_embeddings*** non-trainable\n",
      "Setting module ***roberta.embeddings.position_embeddings*** non-trainable\n",
      "Setting module ***roberta.embeddings.token_type_embeddings*** non-trainable\n",
      "Setting module ***roberta.embeddings.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.embeddings.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder*** non-trainable\n",
      "Setting module ***roberta.encoder.layer*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.0.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.1.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.2.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.3.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.4.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.5.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.6.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.7.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.8.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.9.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.10.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.self*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.self.query*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.self.key*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.self.value*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.self.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.attention.output.dropout*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.intermediate*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.intermediate.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.intermediate.intermediate_act_fn*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.output*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.output.dense*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.output.LayerNorm*** non-trainable\n",
      "Setting module ***roberta.encoder.layer.11.output.dropout*** non-trainable\n"
     ]
    }
   ],
   "source": [
    "for mod_name, mod in model.named_modules():\n",
    "    if len(mod_name) > 0:\n",
    "        if not mod_name.startswith('classifier'):\n",
    "            print(f\"Setting module ***{mod_name}*** non-trainable\")\n",
    "            for param in mod.parameters():\n",
    "                param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202cecff-5f26-41ce-b99a-14739a7c18a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params: 110634256\n",
      "num trainable params: 602896\n"
     ]
    }
   ],
   "source": [
    "print(f\"num params:\", model.num_parameters())\n",
    "print(f\"num trainable params:\", model.num_parameters(only_trainable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3ac7e-7da1-4206-8dcd-eaa4c43f47d2",
   "metadata": {},
   "source": [
    "Etonnant d'avoir 0 paramètres entrainables, peut-être que le classifier n'est pas considéré dans le model.num_parameters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd9e9ed1-0bd3-436d-a324-17d9388d7527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertForSequenceClassification(\n",
      "  (roberta): CamembertModel(\n",
      "    (embeddings): CamembertEmbeddings(\n",
      "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): CamembertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x CamembertLayer(\n",
      "          (attention): CamembertAttention(\n",
      "            (self): CamembertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CamembertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): CamembertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): CamembertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): CamembertClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=16, bias=True)\n",
      "  )\n",
      ")\n",
      "CamembertModel(\n",
      "  (embeddings): CamembertEmbeddings(\n",
      "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): CamembertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x CamembertLayer(\n",
      "        (attention): CamembertAttention(\n",
      "          (self): CamembertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): CamembertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): CamembertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): CamembertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "CamembertEmbeddings(\n",
      "  (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "  (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "  (token_type_embeddings): Embedding(1, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Embedding(32005, 768, padding_idx=1)\n",
      "Embedding(514, 768, padding_idx=1)\n",
      "Embedding(1, 768)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x CamembertLayer(\n",
      "      (attention): CamembertAttention(\n",
      "        (self): CamembertSdpaSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): CamembertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): CamembertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): CamembertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0-11): 12 x CamembertLayer(\n",
      "    (attention): CamembertAttention(\n",
      "      (self): CamembertSdpaSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): CamembertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): CamembertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): CamembertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertLayer(\n",
      "  (attention): CamembertAttention(\n",
      "    (self): CamembertSdpaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): CamembertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): CamembertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): CamembertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertAttention(\n",
      "  (self): CamembertSdpaSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): CamembertSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "CamembertSdpaSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "GELUActivation()\n",
      "CamembertOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "CamembertClassificationHead(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (out_proj): Linear(in_features=768, out_features=16, bias=True)\n",
      ")\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=16, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for module in model.modules(): # Affiche les 'modules' dont les couches\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc4e25a-a361-463f-ae56-0a2203c5a956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc70016c-0968-46e3-8b14-987a841b7efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea299e-03bb-47ff-a2ad-c3f17ecbc77b",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "- Assurez-vous que vos données sont sous forme de texte et de labels multi-labels (par exemple, une liste de listes de labels).\n",
    "- Tokenisez les textes et convertissez les labels en tenseurs PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "667bf282-a665-44a8-83e8-3ed8389f276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Stop #ViolenceAgainstWomen!\\n\\n#OrangeTheWorld \\n\\n📍 Parlement européen, Strasbourg 🧡'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data_raw['data.text'].tolist()\n",
    "print(len(texts))\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8ba14ad-bc61-497f-a955-85e251a75893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(1263, 16))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_arr = np.zeros((len(texts), 16))\n",
    "labels_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a783299f-94ff-4edc-8fdd-c1a472df0f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11], [11], [8, 3], [8, 3, 11], [8, 7]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_raw = data_raw['topic_idx'].tolist()\n",
    "labels_ints = []\n",
    "for labs in labels_raw:\n",
    "    labels_ints.append([int(value) for value in labs.replace('[', '').replace(']', '').split(',')])\n",
    "labels_ints[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e88eee05-b419-4803-951c-f70d323dd6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], shape=(1263, 16))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(labels_arr.shape[0]):\n",
    "    labs = labels_ints[i]\n",
    "    for value in labs:\n",
    "        labels_arr[i, value] = 1\n",
    "labels_arr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "585c76c9-c591-4358-a007-bcd8d4c21a56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list = labels_arr.tolist()\n",
    "labels_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d448a9-14b0-48b5-870c-87924aad3d91",
   "metadata": {},
   "source": [
    "### Split train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64ae3dfa-8fb4-4be6-9dad-ed550453c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille jeu d'entraînement = 909\n",
      "Taille jeu de validation = 101\n",
      "Taille jeu de test = 253\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(texts, labels_list, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Taille jeu d'entraînement = {len(X_train)}\")\n",
    "print(f\"Taille jeu de validation = {len(X_val)}\")\n",
    "print(f\"Taille jeu de test = {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81a8775b-39d1-4bf3-816e-d280ad49451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels_train = torch.tensor(y_train)\n",
    "inputs_val = tokenizer(X_val, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels_val = torch.tensor(y_val)\n",
    "inputs_test = tokenizer(X_test, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0c933-4ad1-48c3-a5da-46f53c27d752",
   "metadata": {},
   "source": [
    "## Création du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a39ea25-fb98-4956-9d2b-1e8ca924aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b4311-524c-4571-80a5-0d0a973f5e02",
   "metadata": {},
   "source": [
    "## Définition de l'optimiseur et de la fonction de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8ab28a7-171f-4696-87a8-d685daaf47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6380ee-4273-4419-bb43-e6aeb4f3f90e",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616d70b-79b9-478c-89a5-c63e5249275d",
   "metadata": {},
   "source": [
    "Fonction de calcul de métrique pour de la classification multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb37f3d2-be2b-4056-8706-ebeccd6c3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred, y_true, threshold=0.5):\n",
    "    # Get probabilities from logits with sigmoid\n",
    "    probabilities = F.sigmoid(y_pred.logits)\n",
    "    \n",
    "    # Apply threshold for binary answer for each class\n",
    "    predictions = (probabilities > threshold).int()\n",
    "    \n",
    "    # Exact Match Ratio\n",
    "    acc = accuracy_score(y_true, predictions, normalize=True, sample_weight=None)\n",
    "    # Hamming loss\n",
    "    hl = hamming_loss(y_true, predictions)\n",
    "    \n",
    "    #\"samples\" applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes \n",
    "    #for each sample in the evaluation data, and returning their (sample_weight-weighted) average.\n",
    "    # Recall\n",
    "    precision = precision_score(y_true=y_true, y_pred=predictions, average='samples')\n",
    "    # Precision\n",
    "    recall = recall_score(y_true=y_true, y_pred=predictions, average='samples')\n",
    "    # F1 Measure\n",
    "    f1 = f1_score(y_true=y_true, y_pred=predictions, average='samples')\n",
    "\n",
    "    return acc, hl, recall, precision, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea0cf0-41a6-4adf-886a-12522e88a41e",
   "metadata": {},
   "source": [
    "Paramètres d'entraînement  \n",
    "\n",
    "Article pour choisir les hyper paramètres : https://mccormickml.com/2019/07/22/BERT-fine-tuning/#42-optimizer--learning-rate-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2e6a9e2-605b-4ec0-a3d8-0f274044f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "threshold_train = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b7312-6d39-4bc3-821b-e7c5ddbf449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "# Initialize metrics\n",
    "dict_metrics = {'accuracy': [],\n",
    "                'hamming': [],\n",
    "                'recall': [],\n",
    "                'precision': [],\n",
    "                'f1': [],\n",
    "                'loss_train': [],\n",
    "                'loss_val': []\n",
    "               }\n",
    "\n",
    "for epoch in range(n_epochs):  # Nombre d'époques\n",
    "    for batch in dataloader:\n",
    "       input_ids, attention_mask, labels = batch\n",
    "       outputs = model(input_ids, attention_mask=attention_mask)\n",
    "       logits = outputs.logits\n",
    "       loss = loss_fn(logits, labels.float())\n",
    "\n",
    "       optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       \n",
    "       # print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    # Training loss\n",
    "    y_pred_train = model(inputs_train['input_ids'], attention_mask=inputs_train['attention_mask'])\n",
    "    loss_train = loss_fn(y_pred_train.logits, labels_train)\n",
    "    \n",
    "    # evaluate model at end of epoch with validation set\n",
    "    y_pred_val = model(inputs_val['input_ids'], attention_mask=inputs_val['attention_mask'])\n",
    "    loss_val = loss_fn(y_pred_val.logits, labels_val)\n",
    "    accuracy_val, hamming_loss_val, recall_val, precision_val, f1score_val = compute_metrics(y_pred_val, y_val, threshold=threshold_train)\n",
    "\n",
    "    # Saving epoch metric for later plotting\n",
    "    dict_metrics['accuracy'].append(accuracy_val)\n",
    "    dict_metrics['hamming'].append(hamming_loss_val)\n",
    "    dict_metrics['recall'].append(recall_val)\n",
    "    dict_metrics['precision'].append(precision_val)\n",
    "    dict_metrics['f1'].append(f1score_val)\n",
    "    dict_metrics['loss_train'].append(loss_train)\n",
    "    dict_metrics['loss_val'].append(loss_val)\n",
    "\n",
    "    print(f\"End of epoch number {epoch}\")\n",
    "    print(f\"   Loss train {loss_train:.4f}\")\n",
    "    print(f\"   Loss val {loss_val:.4f}\")\n",
    "    print(f\"   Accuracy {accuracy_val:.4f}\")\n",
    "    print(f\"   Hamming Loss {hamming_loss_val:.4f}\")\n",
    "    print(f\"   Recall {recall_val:.4f}\")\n",
    "    print(f\"   Precision {precision_val:.4f}\")\n",
    "    print(f\"   F1-Score {f1score_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7724add7-2d20-4d0d-9c70-5cbb1957df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model at end of epoch with validation set\n",
    "y_pred_val = model(inputs_val['input_ids'], attention_mask=inputs_val['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13c33028-4e63-46c8-a14d-096c2d001472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1947, 0.1484, 0.1468,  ..., 0.1845, 0.1443, 0.3537],\n",
       "        [0.1703, 0.1340, 0.1620,  ..., 0.1680, 0.1334, 0.3464],\n",
       "        [0.2050, 0.1521, 0.1663,  ..., 0.1687, 0.1503, 0.3620],\n",
       "        ...,\n",
       "        [0.1736, 0.1344, 0.1290,  ..., 0.1522, 0.1389, 0.3512],\n",
       "        [0.4540, 0.4284, 0.4316,  ..., 0.4425, 0.4483, 0.4839],\n",
       "        [0.1792, 0.1453, 0.1561,  ..., 0.1576, 0.1405, 0.3411]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_val = F.sigmoid(y_pred_val.logits)\n",
    "probabilities_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5ded825-1528-4de4-8474-18ddc3765a97",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(2, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(2, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(2, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(2, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n",
      "tensor(16, dtype=torch.int32)\n",
      "tensor(1, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.4\n",
    "predictions_val = (probabilities_val > threshold).int()\n",
    "for tensor in predictions_val:\n",
    "    print(sum(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e4827bd-1448-48cb-b9d2-64a955c4938b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch number 0\n",
      "   Accuracy 0.2277\n",
      "   Hamming Loss 0.1145\n",
      "   Recall 0.2723\n",
      "   Precision 0.2976\n",
      "   F1-Score 0.2734\n"
     ]
    }
   ],
   "source": [
    "# Exact Match Ratio\n",
    "acc = accuracy_score(y_val, predictions_val, normalize=True, sample_weight=None)\n",
    "# Hamming loss\n",
    "hl = hamming_loss(y_val, predictions_val)\n",
    "\n",
    "#\"samples\" applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes \n",
    "#for each sample in the evaluation data, and returning their (sample_weight-weighted) average.\n",
    "# Recall\n",
    "precision = precision_score(y_true=y_val, y_pred=predictions_val, average='samples')\n",
    "# Precision\n",
    "recall = recall_score(y_true=y_val, y_pred=predictions_val, average='samples')\n",
    "# F1 Measure\n",
    "f1 = f1_score(y_true=y_val, y_pred=predictions_val, average='samples')\n",
    "\n",
    "print(f\"End of epoch number {epoch}\")\n",
    "print(f\"   Accuracy {acc:.4f}\")\n",
    "print(f\"   Hamming Loss {hl:.4f}\")\n",
    "print(f\"   Recall {recall:.4f}\")\n",
    "print(f\"   Precision {precision:.4f}\")\n",
    "print(f\"   F1-Score {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e299f-1780-4845-9687-9c13259f061e",
   "metadata": {},
   "source": [
    "### Évaluation du modèle\n",
    "Après l'entraînement, évaluez votre modèle sur un jeu de données de validation pour vérifier ses performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eadede-ef92-439c-b2fb-ecb72a8464a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Code pour évaluer le modèle sur un jeu de données de validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
